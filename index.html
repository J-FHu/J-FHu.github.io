<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jinfan Hu | Ph.D. Candidate @ SIAT, CAS</title>
  <meta name="description" content="Jinfan Hu is a Ph.D. candidate at SIAT, CAS, researching low-level vision and AI interpretability. Supervised by Prof. Chao Dong.">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <style>
    :root {
      --primary-bg: #0f0f1a;
      --secondary-bg: #1a1a2e;
      --accent: #00c8ff;
      --text-primary: #e0e0ff;
      --text-secondary: #a0a0c0;
      --card-bg: #16213e;
      --border-radius: 16px;
      --transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
    }

    [data-theme="light"] {
      --primary-bg: #ffffff;
      --secondary-bg: #f8f9fa;
      --accent: #0d6efd;
      --text-primary: #212529;
      --text-secondary: #6c757d;
      --card-bg: #f8f9fa;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      background: var(--primary-bg);
      color: var(--text-primary);
      font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
      line-height: 1.7;
      overflow-x: hidden;
      transition: var(--transition);
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 2rem;
    }

    /* Header Styles */
    header {
      display: flex;
      align-items: center;
      gap: 3rem;
      margin: 4rem 0 3rem;
      flex-wrap: wrap;
    }

    .profile-img {
      width: 200px;
      height: 200px;
      border-radius: 50%;
      object-fit: cover;
      border: 3px solid var(--accent);
      box-shadow: 0 0 30px rgba(0, 200, 255, 0.3);
      transition: var(--transition);
    }

    .profile-img:hover {
      transform: scale(1.05) rotate(2deg);
      box-shadow: 0 0 40px rgba(0, 200, 255, 0.5);
    }

    .bio {
      flex: 1;
      min-width: 300px;
    }

    h1 {
      font-size: 2.8rem;
      font-weight: 700;
      background: linear-gradient(90deg, var(--accent), #ff00aa);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 0.5rem;
      letter-spacing: -0.5px;
    }

    .affiliation {
      font-size: 1.1rem;
      color: var(--text-secondary);
      margin-bottom: 1.5rem;
      line-height: 1.6;
    }

    .links {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.75rem 1.5rem;
      background: rgba(0, 200, 255, 0.1);
      color: var(--accent);
      text-decoration: none;
      border-radius: 50px;
      font-weight: 500;
      border: 1px solid rgba(0, 200, 255, 0.3);
      transition: var(--transition);
      backdrop-filter: blur(10px);
    }

    .btn:hover {
      background: rgba(0, 200, 255, 0.2);
      transform: translateY(-3px);
      box-shadow: 0 10px 30px rgba(0, 200, 255, 0.2);
    }

    /* Section Styles */
    section {
      margin: 4rem 0;
    }

    h2 {
      font-size: 2.2rem;
      font-weight: 700;
      margin-bottom: 2rem;
      position: relative;
      
      display: inline-block;
    }

    h2::after {
      content: '';
      position: absolute;
      bottom: -8px;
      left: 0;
      width: 70px;
      height: 4px;
      background: linear-gradient(90deg, var(--accent), transparent);
      border-radius: 2px;
    }

    h3 {
      font-size: 1.4rem;
      font-weight: 600;
      margin: 1.5rem 0 0.5rem;
      color: var(--accent);
    }

    p, li {
      margin-bottom: 1rem;
      color: var(--text-primary);
    }

    ul {
      list-style: none;
      padding-left: 1.5rem;
    }

    ul li {
      position: relative;
      padding-left: 1.5rem;
    }

    ul li::before {
      content: '▹';
      position: absolute;
      left: 0;
      color: var(--accent);
      font-size: 0.9rem;
    }

    a {
      color: var(--accent);
      text-decoration: none;
      transition: var(--transition);
      border-bottom: 1px dotted transparent;
    }

    a:hover {
      border-bottom: 1px dotted var(--accent);
      color: #ffffff;
    }

    img {
      max-width: 100%;
      height: auto;
      border-radius: var(--border-radius);
      margin: 1rem 0;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
      /* 图片等比例缩小，避免过大 */
      max-height: 500px;
      object-fit: contain;
    }

    .pub-item {
      background: var(--card-bg);
      padding: 1.5rem;
      border-radius: var(--border-radius);
      margin-bottom: 1.5rem;
      border: 1px solid rgba(255, 255, 255, 0.05);
      transition: var(--transition);
    }

    .pub-item:hover {
      transform: translateY(-5px);
      box-shadow: 0 20px 40px rgba(0, 0, 0, 0.4);
      border-color: rgba(0, 200, 255, 0.2);
    }

    .highlight {
      color: #ff6b6b;
      font-weight: 600;
      /* 修复宽度问题 */
      display: inline-block;
      padding: 0.2rem 0.6rem;
      background: rgba(255, 107, 107, 0.2);
      border-radius: 20px;
      font-size: 0.85rem;
      border: 1px solid rgba(255, 107, 107, 0.3);
      margin-right: 0.5rem;
      margin-top: 0.5rem;
    }

    .tag {
      display: inline-block;
      padding: 0.2rem 0.6rem;
      background: rgba(255, 107, 107, 0.2);
      border-radius: 20px;
      font-size: 0.85rem;
      margin-right: 0.5rem;
      margin-top: 0.5rem;
      border: 1px solid rgba(255, 107, 107, 0.3);
    }

    /* Theme Toggle Button */
    .theme-toggle {
      position: fixed;
      top: 20px;
      right: 20px;
      background: var(--card-bg);
      border: none;
      width: 50px;
      height: 50px;
      border-radius: 50%;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
      z-index: 100;
      transition: var(--transition);
      color: var(--text-primary);
    }

    .theme-toggle:hover {
      transform: scale(1.05);
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
    }

    /* Publications Toggle */
    .pub-section-header {
      display: flex;
      align-items: center;
      gap: 1rem;
      margin-bottom: 2rem;
    }

    .pub-toggle-btn {
      background: none;
      border: none;
      color: var(--accent);
      font-size: 1.2rem;
      font-weight: 600;
      cursor: pointer;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      transition: var(--transition);
    }

    .pub-toggle-btn:hover {
      color: #ffffff;
    }

    .publication-list {
      display: block; /* 默认展开 */
      animation: fadeIn 0.5s ease-in;
    }

    .publication-list.collapsed {
      display: none; /* 收起时隐藏 */
    }

    @keyframes fadeIn {
      from { opacity: 0; }
      to { opacity: 1; }
    }

    /* Floating Book Widget */
    .book-widget {
      position: fixed;
      bottom: 30px;
      right: 30px;
      width: 80px;
      height: 80px;
      z-index: 1000;
      cursor: pointer;
      transition: var(--transition);
    }

    .book-widget:hover {
      transform: scale(1.1) rotate(5deg);
    }

    .book-widget img {
      width: 100%;
      height: 100%;
      border-radius: 12px;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);
    }

    /* Close Button for Book Widget */
    .close-book-btn {
      position: absolute;
      top: -6px;
      right: -6px;
      width: 20px;
      height: 20px;
      background: #ff4444;
      border-radius: 50%;
      color: white;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 12px;
      font-weight: bold;
      cursor: pointer;
      z-index: 1001;
      transition: background 0.3s;
    }

    .close-book-btn:hover {
      background: #cc0000;
    }

    /* Footer */
    footer {
      text-align: center;
      padding: 3rem 2rem 2rem;
      margin-top: 4rem;
      color: var(--text-secondary);
      font-size: 0.9rem;
      border-top: 1px solid rgba(255, 255, 255, 0.05);
    }

    @media (max-width: 768px) {
      header {
        flex-direction: column;
        text-align: center;
      }
      .links {
        justify-content: center;
      }
      .container {
        padding: 1rem;
      }
      h1 {
        font-size: 2.2rem;
      }
    }

    /* Animations */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    main > section {
      animation: fadeInUp 0.8s ease-out forwards;
    }

    main > section:nth-child(1) { animation-delay: 0.1s; }
    main > section:nth-child(2) { animation-delay: 0.2s; }
    main > section:nth-child(3) { animation-delay: 0.3s; }
    main > section:nth-child(4) { animation-delay: 0.4s; }
    main > section:nth-child(5) { animation-delay: 0.5s; }
    main > section:nth-child(6) { animation-delay: 0.6s; }
    main > section:nth-child(7) { animation-delay: 0.7s; }
  </style>
</head>
<body>
  <div class="container">
    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle">
      <i class="fas fa-moon"></i>
    </button>

    <header>
      <img src="https://J-FHu.github.io/images/Hu_small.jpg" alt="Jinfan Hu" class="profile-img" />
      <div class="bio">
        <h1>Jinfan Hu</h1>
        <p class="affiliation">
          Ph.D. Candidate @ <a href="https://www.siat.ac.cn/" target="_blank">SIAT, CAS</a> | Advised by Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a><br>
          Researching Low-level Vision & AI Interpretability.<br><br>
          
        </p>
        <div class="links">
          <a href="mailto:jf.hu1@siat.ac.cn" class="btn"><i class="fas fa-envelope"></i> Email</a>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=hT-EiJEAAAAJ" class="btn" target="_blank"><i class="fab fa-google"></i> Scholar</a>
          <a href="https://github.com/J-FHu" class="btn" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </div>
      </div>
    </header>

    <main>
      <!-- Biography -->
      <section>
        <h2>📚 Biography</h2>
        <p><strong>Jinfan Hu</strong> is pursuing his Ph.D. degree in <a href="http://xpixel.group/index.html" target="_blank">XPixel Group</a>, Shenzhen Institute of Advanced Technology (<a href="https://www.siat.ac.cn/" target="_blank">SIAT</a>), Chinese Academy of Sciences (<a href="https://english.cas.cn/index.shtml" target="_blank">CAS</a>). He is now supervised by Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a>. He obtained his Bachelor and Master degrees in Mathematics (supervised by Prof. <a href="http://www.math.uestc.edu.cn/info/1081/2041.htm" target="_blank">Ting-Zhu Huang</a> and Prof. <a href="https://liangjiandeng.github.io/" target="_blank">Liang-Jian Deng</a>) from the University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>), Chengdu, China in 2019 and 2022. His research interests include low-level vision (LV) and AI interpretability.</p>
      </section>

<!-- Headlines -->
<section>
  <h2>🔥 Headlines</h2>

  <!-- CEM Paper -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap;">
    <div style="flex: 1; min-width: 300px;">
      <h3>[03/2025] 🎉🎉🎉 One paper is accepted by TPAMI</h3>
      <p>We propose a model/task-agnostic interpreting method (<a href="https://arxiv.org/abs/2407.19789" target="_blank">CEM</a>) for low-level vision models, bringing causality analysis into the field. We hope this work contributes to a deeper understanding and enhancement of low-level vision models!</p>
      <p><strong>Key Insight:</strong> Correlation does not imply causation — only causal analysis reveals why.</p>
    </div>
    <div style="flex: 1; min-width: 300px;">
      <img src="https://J-FHu.github.io/images/teaser_CEM.png" alt="CEM Teaser" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
    </div>
  </div>

  <!-- Generalization Paper -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap; flex-direction: row-reverse;">
    <div style="flex: 1; min-width: 300px;">
      <h3>[02/2025] 🎉🎉🎉 We revisit the generalization problem of low-level vision models</h3>
      <p>This paper demonstrates that the common strategy of blindly expanding the training set is ineffective. Instead, the key to better generalization lies in guiding the network to learn the image content rather than the degradation!</p>
      <p><strong>Takeaway:</strong> Teach networks to see the content, not just remove degradation. </p>
    </div>
    <div style="flex: 1; min-width: 300px;">
      <img src="https://J-FHu.github.io/images/Real_compare.png" alt="Generalization Comparison" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
    </div>
  </div>

  <!-- Book -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap;">
    <div style="flex: 1; min-width: 300px;">
      <h3>[02/2025] 🎉🎉🎉 The book 《底层视觉之美》 has been published</h3>
      <p>Co-authored with my supervisor Prof. Chao Dong. We hope this book provides valuable insights for researchers and enthusiasts in the Low-level Vision field!</p>
      <p><strong>Available on:</strong> <a href="https://item.jd.com/10137558708716.html" target="_blank">JD.com</a></p>
    </div>
    <div style="flex: 1; min-width: 300px;">
      <img src="https://J-FHu.github.io/images/book.jpg" alt="The Beauty of Low-level Vision Book" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
    </div>
  </div>

  <!-- SUPIR -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap; flex-direction: row-reverse;">
    <div style="flex: 1; min-width: 300px;">
      <h3>[01/2024] 🎉🎉🎉 We release a groundbreaking image restoration method (SUPIR)</h3>
      <p>That harnesses the generative prior and the power of model scaling up. Our method demonstrates unprecedented performance in real-world image restoration tasks! Experience the future of image restoration through our official platforms: <a href="https://www.suppixel.cn/home" target="_blank">明犀AI</a> and <a href="https://supir.suppixel.ai/home" target="_blank">SupPixel AI</a>.</p>
      <p><strong>Try it:</strong> Powered by the most advanced AI servers for professional-grade results.</p>
    </div>
    <div style="flex: 1; min-width: 300px;">
      <img src="https://J-FHu.github.io/images/teaser.png" alt="SUPIR Teaser" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
    </div>
  </div>
</section>

      <!-- Publications -->
      <section>
        <div class="pub-section-header">
          <h2>📝 Publications</h2>
          <button class="pub-toggle-btn" id="pubToggle">
            <span id="toggleIcon">▲</span>
          </button>
        </div>
        <div class="publication-list" id="pubList">
          <h3><em>Book</em></h3>
          <div class="pub-item">
            <p>C. Dong, <strong>J. Hu</strong>, <a href="https://item.jd.com/10137558708716.html" target="_blank">《底层视觉之美》</a> (The Beauty of Low-level Vision)，电子工业出版社，2025.</p>
          </div>

          <h3><em>Paper</em></h3>
          <div class="pub-item">
            <p><strong>J. Hu</strong>, J. Gu, S. Yao, F. Yu, Z. Li, Z. You, C. Lu, C. Dong. Interpreting Low-level Vision Models with Causal Effect Maps. <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em></strong>, 2025. <a href="https://github.com/J-FHu/CEM" target="_blank">[Code]</a><a href="https://arxiv.org/abs/2407.19789" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J. Hu<sup>*</sup></strong>, Z. You<sup>*</sup>, J. Gu, K. Zhu, T. Xue, C. Dong. Revisiting the Generalization Problem of Low-level Vision Models Through the Lens of Image Deraining. arXiv, 2025. <a href="https://arxiv.org/abs/2502.12600" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>F. Yu, J. Gu, <strong>J. Hu</strong>, Z. Li, and C. Dong. UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models. <strong><em>International Conference on Learning Representations (ICLR)</em></strong>, 2025. <a href="https://openreview.net/forum?id=uJqKf24HGN" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>F. Yu, J. Gu, Z. Li, <strong>J. Hu</strong>, X. Kong, X. Wang, J. He, Y. Qiao, and C. Dong. Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild. <strong><em>Conference on Computer Vision and Pattern Recognition (CVPR)</em></strong>, 2024. <a href="https://supir.xpixel.group/" target="_blank">[Project Page]</a><a href="https://arxiv.org/abs/2401.13627" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>R. Ran, L.-J. Deng, T.-X. Jiang, <strong>J.-F. Hu</strong>, J. Chanussot, and G. Vivone. GuidedNet: A General CNN Fusion Framework via High-resolution Guidance for Hyperspectral Image Super-resolution. <strong><em>IEEE Transactions on Cybernetics (TCYB)</em></strong>, 2023. <span class="highlight">ESI Highly Cited</span> <a href="https://github.com/Evangelion09/GuidedNet" target="_blank">[Code]</a><a href="https://ieeexplore.ieee.org/abstract/document/10035506/" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>X. Liu<sup>*</sup>, <strong>J. Hu<sup>*</sup></strong>, X. Chen, and C. Dong. UDC-UNet: Under-Display Camera Image Restoration via U-shape Dynamic Network. <strong><em>European Conference on Computer Vision Workshop (ECCVW)</em></strong>, 2022. <a href="https://github.com/J-FHu/UDCUNet" target="_blank">[Code]</a><a href="https://link.springer.com/chapter/10.1007/978-3-031-25072-9_8" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Y.-W. Zhuo, T.-J. Zhang, <strong>J.-F. Hu</strong>, H.-X. Dou, T.-Z. Huang, and L.-J. Deng. Hyper-DSNet: Hyperspectral Pansharpening via A Deep-Shallow Fusion Network with Multi-Detail Extractor and Spectral Attention. <strong><em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em></strong>, 2022. <a href="https://ieeexplore.ieee.org/abstract/document/9870551" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J.-F. Hu</strong>, T.-Z. Huang, L.-J. Deng, H.-X. Dou, D. Hong, and G. Vivone. Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution. <strong><em>IEEE Geoscience and Remote Sensing Letters</em></strong>, 2022. <span class="highlight">ESI Highly Cited</span> <a href="https://github.com/J-FHu/Fusformer" target="_blank">[Code]</a><a href="https://ieeexplore.ieee.org/abstract/document/9841513" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>S. Peng, L.-J. Deng, <strong>J.-F. Hu</strong>, and Y.-W. Zhuo. Source-Adaptive Discriminative Kernels based Network for Remote Sensing Pansharpening. <strong><em>International Joint Conferences on Artificial Intelligence (IJCAI)</em></strong>, 2022. <a href="https://github.com/liangjiandeng/ADKNet" target="_blank">[Code]</a><a href="https://pluto-wei.github.io/papers/2022/peng-ijcai2022.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J.-F. Hu</strong>, T.-Z. Huang, L.-J. Deng, T.-X. Jiang, G. Vivone, and J. Chanussot. Hyperspectral Image Super-Resolution via Deep Spatiospectral Attention Convolutional Neural Networks. <strong><em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</em></strong>, 2021. <span class="highlight">ESI Highly Cited</span> <a href="https://liangjiandeng.github.io/Projects_Res/HSRnet_2021tnnls.html" target="_blank">[Project Page]</a><a href="https://liangjiandeng.github.io/papers/2021/HSRnet_tnnls_2021.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>T. Xu, T.-Z. Huang, L.-J. Deng, X.-L Zhao, and <strong>J.-F. Hu</strong>. Exemplar-based Image Inpainting Using Adaptive Two-Stage Structure-Tensor Based Priority Function and Nonlocal Filtering. <strong><em>Journal of Visual Communication and Image Representation</em></strong>, 2021. <a href="https://www.sciencedirect.com/science/article/abs/pii/S1047320321002893" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Z.-C. Wu, T.-Z. Huang, L.-J. Deng, <strong>J.-F. Hu</strong>, and G. Vivone. VO+ Net: An Adaptive Approach Using Variational Optimization and Deep Learning for Panchromatic Sharpening. <strong><em>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</em></strong>, 2021. <a href="https://liangjiandeng.github.io/Projects_Res/VOFF_2021tgrs.html" target="_blank">[Project page]</a><a href="https://liangjiandeng.github.io/papers/2021/VOFF.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Z.-C. Wu, T.-Z. Huang, L.-J. Deng, G. Vivone, J.-Q Miao, <strong>J.-F. Hu</strong>, and X.-L Zhao. A New Variational Approach Based on Proximal Deep Injection and Gradient Intensity Similarity for Spatio-Spectral Image Fusion. <strong><em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em></strong>, 2020. <a href="https://liangjiandeng.github.io/Projects_Res/DMPIF_2020jstars.html" target="_blank">[Project page]</a><a href="https://liangjiandeng.github.io/papers/2020/dmpif_2020jstars.pdf" target="_blank">[PDF]</a></p>
          </div>
          <p align="left"><small>(<sup>*</sup> Equal contributions)</small></p>
        </div>
      </section>

      <!-- Products -->
      <section>
        <h2>📸 Products</h2>
        <p>For a comprehensive exploration of our technologies, visit our official websites: <a href="https://www.suppixel.cn/home" target="_blank">明犀AI</a> and <a href="https://supir.suppixel.ai/home" target="_blank">SupPixel AI</a>.</p>
      </section>

      <!-- Experiences -->
      <section>
        <h2>📖 Experiences</h2>
        <ul>
          <li>09/2022-present: Ph.D. student in computer science. (Supervisor: Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a>); Shenzhen Institute of Advanced Technology (<a href="https://www.siat.ac.cn/" target="_blank">SIAT</a>), University of Chinese Academy of Sciences (<a href="https://www.ucas.ac.cn/" target="_blank">UCAS</a>)</li>
          <li>09/2019-06/2022: Master student in mathematics. (Supervisor: Prof. <a href="http://www.math.uestc.edu.cn/info/1081/2041.htm" target="_blank">Ting-Zhu Huang</a> and Prof. <a href="https://liangjiandeng.github.io/" target="_blank">Liang-Jian Deng</a>); University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>)</li>
          <li>09/2015-06/2019: Bachelor student in information and computing science; University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>)</li>
        </ul>
      </section>

      <!-- Honors and Awards -->
      <section>
        <h2>🏆 Honors and Awards</h2>
        <ul>
          <li>2nd Place in ECCV <a href="http://mipi-challenge.org/" target="_blank">MIPI 2022 Challenge</a> on UDC image restoration, 2022</li>
          <li>National Scholarship, 2021</li>
          <li>National First Prize of <a href="http://www.mcm.edu.cn/" target="_blank">CUMCM</a>, 2017</li>
        </ul>
      </section>

      <!-- Academic Activities -->
      <section>
        <h2>💬 Academic Activities</h2>
        <p><strong>Peer-Reviewer:</strong></p>
        <ul>
          <li>IEEE Transactions on Pattern Analysis and Machine Intelligence</li>
          <li>IEEE Transactions on Cybernetics</li>
          <li>IEEE Transactions on Geoscience And Remote Sensing</li>
          <li>IEEE Geoscience and Remote Sensing Letters</li>
          <li>IEEE Transactions on Computational Imaging</li>
          <li>...</li>
        </ul>
      </section>
    </main>

    <footer>
      © 2025 Jinfan Hu. Built with QWen.
    </footer>

<!-- Floating Book Widget with Close Button -->
<div class="book-widget" id="bookWidget">
  <div class="close-book-btn" id="closeBookBtn">×</div>
  <a href="https://item.jd.com/10137558708716.html" target="_blank">
    <img src="https://J-FHu.github.io/images/bookicon.jpg" alt="《底层视觉之美》">
  </a>
</div>

<style>
  /* Floating Book Widget */
  .book-widget {
    position: fixed;
    bottom: 30px;
    right: 30px;
    width: 100px; 
    height: 100px; 
    z-index: 1000;
    cursor: pointer;
    transition: var(--transition);
  }

  .book-widget:hover {
    transform: scale(1.1) rotate(5deg);
  }

  .book-widget img {
    width: 100%;
    height: 100%;
    border-radius: 12px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);
  }

  /* Close Button for Book Widget */
  .close-book-btn {
    position: absolute;
    top: -10px;
    right: -10px;
    width: 35px;
    height: 35px;
    background: #ff4444;
    border-radius: 50%;
    color: white;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 18px;
    font-weight: bold;
    cursor: pointer;
    z-index: 1001;
    transition: background 0.3s;
    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  }

  .close-book-btn:hover {
    background: #cc0000;
    transform: scale(1.1);
  }
</style>

    <!-- ClustrMaps Visitor Counter -->
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=5Tnjvlx4pwYiXFQMdiOgvLHFKUuOMdlwnkZD9DMfS6c'></script>

    <!-- Main JavaScript -->
    <script>
      // Theme Toggle Script
      const themeToggle = document.getElementById('themeToggle');
      const body = document.body;

      // Check for saved theme preference
      const savedTheme = localStorage.getItem('theme') || 'dark';
      if (savedTheme === 'light') {
        body.setAttribute('data-theme', 'light');
        themeToggle.innerHTML = '<i class="fas fa-sun"></i>';
      }

      themeToggle.addEventListener('click', () => {
        const currentTheme = body.getAttribute('data-theme');
        if (currentTheme === 'light') {
          body.removeAttribute('data-theme');
          localStorage.setItem('theme', 'dark');
          themeToggle.innerHTML = '<i class="fas fa-moon"></i>';
        } else {
          body.setAttribute('data-theme', 'light');
          localStorage.setItem('theme', 'light');
          themeToggle.innerHTML = '<i class="fas fa-sun"></i>';
        }
      });

      // Publication Toggle Script
      const pubToggle = document.getElementById('pubToggle');
  const pubList = document.getElementById('pubList');

  pubToggle.addEventListener('click', () => {
    pubList.classList.toggle('collapsed'); 
    if (pubList.classList.contains('collapsed')) {
      pubToggle.innerHTML = '<span id="toggleIcon">▼</span>';
    } else {
      pubToggle.innerHTML = '<span id="toggleIcon">▲</span>';
    }
  });

      // Book Widget Close Script
      const closeBookBtn = document.getElementById('closeBookBtn');
      const bookWidget = document.getElementById('bookWidget');

      closeBookBtn.addEventListener('click', function(e) {
        e.stopPropagation(); //
        bookWidget.style.display = 'none';
      });

      // 页面加载时检查是否之前已关闭
      if (localStorage.getItem('bookWidgetClosed') === 'true') {
        bookWidget.style.display = 'none';
      }
    </script>
  </div>
</body>
</html>
