<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jinfan Hu | Ph.D. Candidate @ SIAT, CAS</title>
  <meta name="description" content="Jinfan Hu is a Ph.D. candidate at SIAT, CAS, researching low-level vision and AI interpretability. Supervised by Prof. Chao Dong.">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <style>
    :root {
      --primary-bg: #0f0f1a;
      --secondary-bg: #1a1a2e;
      --accent: #00c8ff;
      --text-primary: #e0e0ff;
      --text-secondary: #a0a0c0;
      --card-bg: #16213e;
      --border-radius: 16px;
      --transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
    }

    [data-theme="light"] {
      --primary-bg: #ffffff;
      --secondary-bg: #f8f9fa;
      --accent: #0d6efd;
      --text-primary: #212529;
      --text-secondary: #6c757d;
      --card-bg: #f8f9fa;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      background: var(--primary-bg);
      color: var(--text-primary);
      font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
      line-height: 1.7;
      overflow-x: hidden;
      transition: var(--transition);
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 2rem;
    }

    /* Header Styles */
    header {
      display: flex;
      align-items: center;
      gap: 3rem;
      margin: 4rem 0 3rem;
      flex-wrap: wrap;
    }

    .profile-img {
      width: 200px;
      height: 200px;
      border-radius: 50%;
      object-fit: cover;
      border: 3px solid var(--accent);
      box-shadow: 0 0 30px rgba(0, 200, 255, 0.3);
      transition: var(--transition);
    }

    .profile-img:hover {
      transform: scale(1.05) rotate(2deg);
      box-shadow: 0 0 40px rgba(0, 200, 255, 0.5);
    }

    .bio {
      flex: 1;
      min-width: 300px;
    }

    h1 {
      font-size: 2.8rem;
      font-weight: 700;
      background: linear-gradient(90deg, var(--accent), #ff00aa);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 0.5rem;
      letter-spacing: -0.5px;
    }

    .affiliation {
      font-size: 1.1rem;
      color: var(--text-secondary);
      margin-bottom: 1.5rem;
      line-height: 1.6;
    }

    .links {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.75rem 1.5rem;
      background: rgba(0, 200, 255, 0.1);
      color: var(--accent);
      text-decoration: none;
      border-radius: 50px;
      font-weight: 500;
      border: 1px solid rgba(0, 200, 255, 0.3);
      transition: var(--transition);
      backdrop-filter: blur(10px);
    }

    .btn:hover {
      background: rgba(0, 200, 255, 0.2);
      transform: translateY(-3px);
      box-shadow: 0 10px 30px rgba(0, 200, 255, 0.2);
    }

    /* Section Styles */
    section {
      margin: 4rem 0;
    }

    h2 {
      font-size: 2.2rem;
      font-weight: 700;
      margin-bottom: 2rem;
      position: relative;
      display: inline-block;
    }

    h2::after {
      content: '';
      position: absolute;
      bottom: -8px;
      left: 0;
      width: 70px;
      height: 4px;
      background: linear-gradient(90deg, var(--accent), transparent);
      border-radius: 2px;
    }

    h3 {
      font-size: 1.4rem;
      font-weight: 600;
      margin: 1.5rem 0 0.5rem;
      color: var(--accent);
    }

    p, li {
      margin-bottom: 1rem;
      color: var(--text-primary);
    }

    ul {
      list-style: none;
      padding-left: 1.5rem;
    }

    ul li {
      position: relative;
      padding-left: 1.5rem;
    }

    ul li::before {
      content: 'â–¹';
      position: absolute;
      left: 0;
      color: var(--accent);
      font-size: 0.9rem;
    }

    a {
      color: var(--accent);
      text-decoration: none;
      transition: var(--transition);
      border-bottom: 1px dotted transparent;
    }

    a:hover {
      border-bottom: 1px dotted var(--accent);
      color: #ffffff;
    }

    img {
      max-width: 100%;
      height: auto;
      border-radius: var(--border-radius);
      margin: 1rem 0;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
    }

    .pub-item {
      background: var(--card-bg);
      padding: 1.5rem;
      border-radius: var(--border-radius);
      margin-bottom: 1.5rem;
      border: 1px solid rgba(255, 255, 255, 0.05);
      transition: var(--transition);
    }

    .pub-item:hover {
      transform: translateY(-5px);
      box-shadow: 0 20px 40px rgba(0, 0, 0, 0.4);
      border-color: rgba(0, 200, 255, 0.2);
    }

    .highlight {
      color: #ff6b6b;
      font-weight: 600;
    }

    .tag {
      display: inline-block;
      padding: 0.2rem 0.6rem;
      background: rgba(255, 107, 107, 0.2);
      border-radius: 20px;
      font-size: 0.85rem;
      margin-right: 0.5rem;
      margin-top: 0.5rem;
      border: 1px solid rgba(255, 107, 107, 0.3);
    }

    /* Floating Book Widget */
    .book-widget {
      position: fixed;
      bottom: 30px;
      right: 30px;
      width: 80px;
      height: 80px;
      z-index: 1000;
      cursor: pointer;
      transition: var(--transition);
    }

    .book-widget:hover {
      transform: scale(1.1) rotate(5deg);
    }

    .book-widget img {
      width: 100%;
      height: 100%;
      border-radius: 12px;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);
    }

    /* Footer */
    footer {
      text-align: center;
      padding: 3rem 2rem 2rem;
      margin-top: 4rem;
      color: var(--text-secondary);
      font-size: 0.9rem;
      border-top: 1px solid rgba(255, 255, 255, 0.05);
    }

    /* Theme Toggle Button */
    .theme-toggle {
      position: fixed;
      top: 20px;
      right: 20px;
      background: var(--card-bg);
      border: none;
      width: 50px;
      height: 50px;
      border-radius: 50%;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
      z-index: 100;
      transition: var(--transition);
      color: var(--text-primary);
    }

    .theme-toggle:hover {
      transform: scale(1.05);
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
    }

    /* Publications Accordion */
    .publications-section {
      position: relative;
    }

    .publication-toggle {
      display: block;
      margin-bottom: 1rem;
      font-weight: 600;
      color: var(--accent);
      cursor: pointer;
      transition: var(--transition);
      border-bottom: 1px solid rgba(255, 255, 255, 0.1);
    }

    .publication-toggle:hover {
      color: #ffffff;
      border-bottom-color: var(--accent);
    }

    .publication-list {
      display: none;
      margin-top: 1rem;
    }

    .publication-list.visible {
      display: block;
    }

    @media (max-width: 768px) {
      header {
        flex-direction: column;
        text-align: center;
      }
      .links {
        justify-content: center;
      }
      .container {
        padding: 1rem;
      }
      h1 {
        font-size: 2.2rem;
      }
    }

    /* Animations */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    main > * {
      animation: fadeInUp 0.8s ease-out forwards;
    }

    main > *:nth-child(1) { animation-delay: 0.1s; }
    main > *:nth-child(2) { animation-delay: 0.2s; }
    main > *:nth-child(3) { animation-delay: 0.3s; }
    main > *:nth-child(4) { animation-delay: 0.4s; }
    main > *:nth-child(5) { animation-delay: 0.5s; }
    main > *:nth-child(6) { animation-delay: 0.6s; }
    main > *:nth-child(7) { animation-delay: 0.7s; }
  </style>
</head>
<body>
  <div class="container">
    <!-- Theme Toggle Button -->
    <button class="theme-toggle" id="themeToggle">
      <i class="fas fa-moon"></i>
    </button>

    <header>
      <img src="https://J-FHu.github.io/images/Hu.jpg" alt="Jinfan Hu" class="profile-img" />
      <div class="bio">
        <h1>Jinfan Hu</h1>
        <p class="affiliation">
          Ph.D. Candidate @ <a href="https://www.siat.ac.cn/" target="_blank">SIAT, CAS</a> | Advised by Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a><br>
          Researching Low-level Vision & AI Interpretability.<br><br>
          <strong>Email:</strong> jf.hu1@siat.ac.cn<br>
          <strong>Follow Me:</strong> <a href="https://scholar.google.com/citations?hl=zh-CN&user=hT-EiJEAAAAJ" target="_blank">Google Scholar</a> &emsp; <a href="https://github.com/J-FHu" target="_blank">Github</a>
        </p>
        <div class="links">
          <a href="mailto:jf.hu1@siat.ac.cn" class="btn"><i class="fas fa-envelope"></i> Email</a>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=hT-EiJEAAAAJ" class="btn" target="_blank"><i class="fab fa-google"></i> Scholar</a>
          <a href="https://github.com/J-FHu" class="btn" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </div>
      </div>
    </header>

    <main>
      <!-- Biography -->
      <section>
        <h2>ğŸ“š Biography</h2>
        <p><strong>Jinfan Hu</strong> is pursuing his Ph.D. degree in <a href="http://xpixel.group/index.html" target="_blank">XPixel Group</a>, Shenzhen Institute of Advanced Technology (<a href="https://www.siat.ac.cn/" target="_blank">SIAT</a>), Chinese Academy of Sciences (<a href="https://english.cas.cn/index.shtml" target="_blank">CAS</a>). He is now supervised by Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a>. He obtained his Bachelor and Master degrees in Mathematics (supervised by Prof. <a href="http://www.math.uestc.edu.cn/info/1081/2041.htm" target="_blank">Ting-Zhu Huang</a> and Prof. <a href="https://liangjiandeng.github.io/" target="_blank">Liang-Jian Deng</a>) from the University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>), Chengdu, China in 2019 and 2022. His research interests include low-level vision (LV) and AI interpretability.</p>
      </section>

      <!-- Headlines -->
      <section>
        <h2>ğŸ”¥ Headlines</h2>
        
        <div class="pub-item">
          <h3>[03/2025] ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by TPAMI</h3>
          <p>We propose a model/task-agnostic interpreting method (<a href="https://arxiv.org/abs/2407.19789" target="_blank">CEM</a>) for low-level vision models, bringing causality analysis into the field. We hope this work contributes to a deeper understanding and enhancement of low-level vision models!</p>
          <img src="https://J-FHu.github.io/images/teaser_CEM.png" alt="CEM Teaser">
        </div>

        <div class="pub-item">
          <h3>[02/2025] ğŸ‰ğŸ‰ğŸ‰ We revisit the generalization problem of low-level vision models</h3>
          <p>We demonstrate that the common strategy of blindly expanding the training set is ineffective. Instead, the key to better generalization lies in guiding the network to learn the image content rather than the degradation!</p>
          <img src="https://J-FHu.github.io/images/Real_compare.png" alt="Generalization Comparison">
        </div>

        <div class="pub-item">
          <h3>[02/2025] ğŸ‰ğŸ‰ğŸ‰ The book ã€Šåº•å±‚è§†è§‰ä¹‹ç¾ã€‹ has been published</h3>
          <p>Co-authored with my supervisor Prof. Chao Dong. We hope this book provides valuable insights for researchers and enthusiasts in the Low-level Vision field!</p>
          <img src="https://J-FHu.github.io/images/book.jpg" alt="The Beauty of Low-level Vision Book">
        </div>

        <div class="pub-item">
          <h3>[01/2024] ğŸ‰ğŸ‰ğŸ‰ We release a groundbreaking image restoration method (SUPIR)</h3>
          <p>That harnesses generative prior and the power of model scaling up. Our method demonstrates unprecedented performance in real-world image restoration tasks! Experience the future of image restoration through our official platforms: <a href="https://www.suppixel.cn/home" target="_blank">æ˜çŠ€AI</a> and <a href="https://supir.suppixel.ai/home" target="_blank">SupPixel AI</a>.</p>
          <img src="https://J-FHu.github.io/images/teaser.png" alt="SUPIR Teaser">
        </div>
      </section>

      <!-- Publications -->
      <section class="publications-section">
        <div class="publication-toggle" id="pubToggle">å±•å¼€æ‰€æœ‰å‡ºç‰ˆç‰©</div>
        <div class="publication-list" id="pubList">
          <h3><em>Book</em></h3>
          <div class="pub-item">
            <p>C. Dong, <strong>J. Hu</strong>, <a href="https://item.jd.com/10137558708716.html" target="_blank">ã€Šåº•å±‚è§†è§‰ä¹‹ç¾ã€‹</a> (The Beauty of Low-level Vision), 2025.</p>
          </div>

          <h3><em>Paper</em></h3>
          <div class="pub-item">
            <p><strong>J. Hu</strong>, J. Gu, S. Yao, F. Yu, Z. Li, Z. You, C. Lu, C. Dong. Interpreting Low-level Vision Models with Causal Effect Maps. <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em></strong>, 2025. <a href="https://github.com/J-FHu/CEM" target="_blank">[Code]</a><a href="https://arxiv.org/abs/2407.19789" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J. Hu<sup>*</sup></strong>, Z. You<sup>*</sup>, J. Gu, K. Zhu, T. Xue, C. Dong. Revisiting the Generalization Problem of Low-level Vision Models Through the Lens of Image Deraining. arXiv, 2025. <a href="https://arxiv.org/abs/2502.12600" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>F. Yu, J. Gu, <strong>J. Hu</strong>, Z. Li, and C. Dong. UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models. <strong><em>International Conference on Learning Representations (ICLR)</em></strong>, 2025. <a href="https://openreview.net/forum?id=uJqKf24HGN" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>F. Yu, J. Gu, Z. Li, <strong>J. Hu</strong>, X. Kong, X. Wang, J. He, Y. Qiao, and C. Dong. Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild. <strong><em>Conference on Computer Vision and Pattern Recognition (CVPR)</em></strong>, 2024. <a href="https://supir.xpixel.group/" target="_blank">[Project Page]</a><a href="https://arxiv.org/abs/2401.13627" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>R. Ran, L.-J. Deng, T.-X. Jiang, <strong>J.-F. Hu</strong>, J. Chanussot, and G. Vivone. GuidedNet: A General CNN Fusion Framework via High-resolution Guidance for Hyperspectral Image Super-resolution. <strong><em>IEEE Transactions on Cybernetics (TCYB)</em></strong>, 2023. <span class="highlight">ESI Highly Cited</span> <a href="https://github.com/Evangelion09/GuidedNet" target="_blank">[Code]</a><a href="https://ieeexplore.ieee.org/abstract/document/10035506/" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>X. Liu<sup>*</sup>, <strong>J. Hu<sup>*</sup></strong>, X. Chen, and C. Dong. UDC-UNet: Under-Display Camera Image Restoration via U-shape Dynamic Network. <strong><em>European Conference on Computer Vision Workshop (ECCVW)</em></strong>, 2022. <a href="https://github.com/J-FHu/UDCUNet" target="_blank">[Code]</a><a href="https://link.springer.com/chapter/10.1007/978-3-031-25072-9_8" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Y.-W. Zhuo, T.-J. Zhang, <strong>J.-F. Hu</strong>, H.-X. Dou, T.-Z. Huang, and L.-J. Deng. Hyper-DSNet: Hyperspectral Pansharpening via A Deep-Shallow Fusion Network with Multi-Detail Extractor and Spectral Attention. <strong><em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em></strong>, 2022. <a href="https://ieeexplore.ieee.org/abstract/document/9870551" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J.-F. Hu</strong>, T.-Z. Huang, L.-J. Deng, H.-X. Dou, D. Hong, and G. Vivone. Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution. <strong><em>IEEE Geoscience and Remote Sensing Letters</em></strong>, 2022. <span class="highlight">ESI Highly Cited</span> <a href="https://github.com/J-FHu/Fusformer" target="_blank">[Code]</a><a href="https://ieeexplore.ieee.org/abstract/document/9841513" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>S. Peng, L.-J. Deng, <strong>J.-F. Hu</strong>, and Y.-W. Zhuo. Source-Adaptive Discriminative Kernels based Network for Remote Sensing Pansharpening. <strong><em>International Joint Conferences on Artificial Intelligence (IJCAI)</em></strong>, 2022. <a href="https://github.com/liangjiandeng/ADKNet" target="_blank">[Code]</a><a href="https://pluto-wei.github.io/papers/2022/peng-ijcai2022.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J.-F. Hu</strong>, T.-Z. Huang, L.-J. Deng, T.-X. Jiang, G. Vivone, and J. Chanussot. Hyperspectral Image Super-Resolution via Deep Spatiospectral Attention Convolutional Neural Networks. <strong><em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</em></strong>, 2021. <span class="highlight">ESI Highly Cited</span> <a href="https://liangjiandeng.github.io/Projects_Res/HSRnet_2021tnnls.html" target="_blank">[Project Page]</a><a href="https://liangjiandeng.github.io/papers/2021/HSRnet_tnnls_2021.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>T. Xu, T.-Z. Huang, L.-J. Deng, X.-L Zhao, and <strong>J.-F. Hu</strong>. Exemplar-based Image Inpainting Using Adaptive Two-Stage Structure-Tensor Based Priority Function and Nonlocal Filtering. <strong><em>Journal of Visual Communication and Image Representation</em></strong>, 2021. <a href="https://www.sciencedirect.com/science/article/abs/pii/S1047320321002893" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Z.-C. Wu, T.-Z. Huang, L.-J. Deng, <strong>J.-F. Hu</strong>, and G. Vivone. VO+ Net: An Adaptive Approach Using Variational Optimization and Deep Learning for Panchromatic Sharpening. <strong><em>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</em></strong>, 2021. <a href="https://liangjiandeng.github.io/Projects_Res/VOFF_2021tgrs.html" target="_blank">[Project page]</a><a href="https://liangjiandeng.github.io/papers/2021/VOFF.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Z.-C. Wu, T.-Z. Huang, L.-J. Deng, G. Vivone, J.-Q Miao, <strong>J.-F. Hu</strong>, and X.-L Zhao. A New Variational Approach Based on Proximal Deep Injection and Gradient Intensity Similarity for Spatio-Spectral Image Fusion. <strong><em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em></strong>, 2020. <a href="https://liangjiandeng.github.io/Projects_Res/DMPIF_2020jstars.html" target="_blank">[Project page]</a><a href="https://liangjiandeng.github.io/papers/2020/dmpif_2020jstars.pdf" target="_blank">[PDF]</a></p>
          </div>
          <p align="left"><small>(<sup>*</sup> Equal contributions)</small></p>
        </div>
      </section>

      <!-- Products -->
      <section>
        <h2>ğŸ“¸ Products</h2>
        <p>For a comprehensive exploration of our technologies, visit our official websites: <a href="https://www.suppixel.cn/home" target="_blank">æ˜çŠ€AI</a> and <a href="https://supir.suppixel.ai/home" target="_blank">SupPixel AI</a>.</p>
      </section>

      <!-- Experiences -->
      <section>
        <h2>ğŸ“– Experiences</h2>
        <ul>
          <li>09/2022-present: Ph.D. student in computer science. (Supervisor: Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a>); Shenzhen Institute of Advanced Technology (<a href="https://www.siat.ac.cn/" target="_blank">SIAT</a>), University of Chinese Academy of Sciences (<a href="https://www.ucas.ac.cn/" target="_blank">UCAS</a>)</li>
          <li>09/2019-06/2022: Master student in mathematics. (Supervisor: Prof. <a href="http://www.math.uestc.edu.cn/info/1081/2041.htm" target="_blank">Ting-Zhu Huang</a> and Prof. <a href="https://liangjiandeng.github.io/" target="_blank">Liang-Jian Deng</a>); University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>)</li>
          <li>09/2015-06/2019: Bachelor student in information and computing science; University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>)</li>
        </ul>
      </section>

      <!-- Honors and Awards -->
      <section>
        <h2>ğŸ† Honors and Awards</h2>
        <ul>
          <li>2nd Place in ECCV <a href="http://mipi-challenge.org/" target="_blank">MIPI 2022 Challenge</a> on UDC image restoration, 2022</li>
          <li>National Scholarship, 2021</li>
          <li>National First Prize of <a href="http://www.mcm.edu.cn/" target="_blank">CUMCM</a>, 2017</li>
        </ul>
      </section>

      <!-- Academic Activities -->
      <section>
        <h2>ğŸ’¬ Academic Activities</h2>
        <p><strong>Peer-Reviewer:</strong></p>
        <ul>
          <li>IEEE Transactions on Pattern Analysis and Machine Intelligence</li>
          <li>IEEE Transactions on Cybernetics</li>
          <li>IEEE Transactions on Geoscience And Remote Sensing</li>
          <li>IEEE Geoscience and Remote Sensing Letters</li>
          <li>IEEE Transactions on Computational Imaging</li>
          <li>...</li>
        </ul>
      </section>
    </main>

    <footer>
      Â© 2025 Jinfan Hu. Built with science and passion.
    </footer>

    <!-- Floating Book Widget -->
    <a href="https://item.jd.com/10137558708716.html" target="_blank" class="book-widget">
      <img src="https://J-FHu.github.io/images/bookicon.jpg" alt="ã€Šåº•å±‚è§†è§‰ä¹‹ç¾ã€‹">
    </a>
  </div>

  <script>
    // Theme Toggle Script
    const themeToggle = document.getElementById('themeToggle');
    const body = document.body;

    // Check for saved theme preference
    const savedTheme = localStorage.getItem('theme') || 'dark';
    if (savedTheme === 'light') {
      body.setAttribute('data-theme', 'light');
      themeToggle.innerHTML = '<i class="fas fa-sun"></i>';
    }

    themeToggle.addEventListener('click', () => {
      const currentTheme = body.getAttribute('data-theme');
      if (currentTheme === 'light') {
        body.removeAttribute('data-theme');
        localStorage.setItem('theme', 'dark');
        themeToggle.innerHTML = '<i class="fas fa-moon"></i>';
      } else {
        body.setAttribute('data-theme', 'light');
        localStorage.setItem('theme', 'light');
        themeToggle.innerHTML = '<i class="fas fa-sun"></i>';
      }
    });

    // Publication Toggle Script
    const pubToggle = document.getElementById('pubToggle');
    const pubList = document.getElementById('pubList');

    pubToggle.addEventListener('click', () => {
      pubList.classList.toggle('visible');
      if (pubList.classList.contains('visible')) {
        pubToggle.textContent = 'æ”¶èµ·å‡ºç‰ˆç‰©';
      } else {
        pubToggle.textContent = 'å±•å¼€æ‰€æœ‰å‡ºç‰ˆç‰©';
      }
    });
  </script>
</body>
</html>
