<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Jinfan Hu | Ph.D. Candidate @ SIAT, CAS</title>
  <meta name="description" content="Jinfan Hu is a Ph.D. candidate at SIAT, CAS, researching low-level vision and AI interpretability. Supervised by Prof. Chao Dong.">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <style>
    :root {
      --primary-bg: #0f0f1a;
      --secondary-bg: #1a1a2e;
      --accent: #00c8ff;
      --text-primary: #e0e0ff;
      --text-secondary: #a0a0c0;
      --card-bg: #16213e;
      --border-radius: 16px;
      --transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
    }

    [data-theme="light"] {
      --primary-bg: #ffffff;
      --secondary-bg: #f8f9fa;
      --accent: #0d6efd;
      --text-primary: #212529;
      --text-secondary: #6c757d;
      --card-bg: #f8f9fa;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      background: var(--primary-bg);
      color: var(--text-primary);
      font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
      line-height: 1.7;
      overflow-x: hidden;
      transition: var(--transition);
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 2rem;
    }

    /* Header Styles */
    header {
      display: flex;
      align-items: center;
      gap: 3rem;
      margin: 4rem 0 3rem;
      flex-wrap: wrap;
    }

    .profile-img {
      width: 200px;
      height: 200px;
      border-radius: 50%;
      object-fit: cover;
      border: 3px solid var(--accent);
      box-shadow: 0 0 30px rgba(0, 200, 255, 0.3);
      transition: var(--transition);
    }

    .profile-img:hover {
      transform: scale(1.05) rotate(2deg);
      box-shadow: 0 0 40px rgba(0, 200, 255, 0.5);
    }

    .bio {
      flex: 1;
      min-width: 300px;
    }

    h1 {
      font-size: 2.8rem;
      font-weight: 700;
      background: linear-gradient(90deg, var(--accent), #ff00aa);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 0.5rem;
      letter-spacing: -0.5px;
    }

    .affiliation {
      font-size: 1.1rem;
      color: var(--text-secondary);
      margin-bottom: 1.5rem;
      line-height: 1.6;
    }

    .links {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.75rem 1.5rem;
      background: rgba(0, 200, 255, 0.1);
      color: var(--accent);
      text-decoration: none;
      border-radius: 50px;
      font-weight: 500;
      border: 1px solid rgba(0, 200, 255, 0.3);
      transition: var(--transition);
      backdrop-filter: blur(10px);
    }

    .btn:hover {
      background: rgba(0, 200, 255, 0.2);
      transform: translateY(-3px);
      box-shadow: 0 10px 30px rgba(0, 200, 255, 0.2);
    }

    /* Section Styles */
    section {
      margin: 4rem 0;
    }

    h2 {
      font-size: 2.2rem;
      font-weight: 700;
      margin-bottom: 2rem;
      position: relative;
      
      display: inline-block;
    }

    h2::after {
      content: '';
      position: absolute;
      bottom: -8px;
      left: 0;
      width: 70px;
      height: 4px;
      background: linear-gradient(90deg, var(--accent), transparent);
      border-radius: 2px;
    }

    h3 {
      font-size: 1.4rem;
      font-weight: 600;
      margin: 1.5rem 0 0.5rem;
      color: var(--accent);
    }

    p, li {
      margin-bottom: 1rem;
      color: var(--text-primary);
    }

    ul {
      list-style: none;
      padding-left: 1.5rem;
    }

    ul li {
      position: relative;
      padding-left: 1.5rem;
    }

    ul li::before {
      content: 'â–¹';
      position: absolute;
      left: 0;
      color: var(--accent);
      font-size: 0.9rem;
    }

    a {
      color: var(--accent);
      text-decoration: none;
      transition: var(--transition);
      border-bottom: 1px dotted transparent;
    }

    a:hover {
      border-bottom: 1px dotted var(--accent);
      color: #ffffff;
    }

    img {
      max-width: 100%;
      height: auto;
      border-radius: var(--border-radius);
      margin: 1rem 0;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
      /* å›¾ç‰‡ç­‰æ¯”ä¾‹ç¼©å°ï¼Œé¿å…è¿‡å¤§ */
      max-height: 500px;
      object-fit: contain;
    }

    .pub-item {
      background: var(--card-bg);
      padding: 1.5rem;
      border-radius: var(--border-radius);
      margin-bottom: 1.5rem;
      border: 1px solid rgba(255, 255, 255, 0.05);
      transition: var(--transition);
    }

    .pub-item:hover {
      transform: translateY(-5px);
      box-shadow: 0 20px 40px rgba(0, 0, 0, 0.4);
      border-color: rgba(0, 200, 255, 0.2);
    }

    .highlight {
      color: #ff6b6b;
      font-weight: 600;
      /* ä¿®å¤å®½åº¦é—®é¢˜ */
      display: inline-block;
      padding: 0.2rem 0.6rem;
      background: rgba(255, 107, 107, 0.2);
      border-radius: 20px;
      font-size: 0.85rem;
      border: 1px solid rgba(255, 107, 107, 0.3);
      margin-right: 0.5rem;
      margin-top: 0.5rem;
    }

    .tag {
      display: inline-block;
      padding: 0.2rem 0.6rem;
      background: rgba(255, 107, 107, 0.2);
      border-radius: 20px;
      font-size: 0.85rem;
      margin-right: 0.5rem;
      margin-top: 0.5rem;
      border: 1px solid rgba(255, 107, 107, 0.3);
    }

    /* Publications Toggle */
    .pub-section-header {
      display: flex;
      align-items: center;
      gap: 1rem;
      margin-bottom: 2rem;
    }

    .pub-toggle-btn {
      background: none;
      border: none;
      color: var(--accent);
      font-size: 1.2rem;
      font-weight: 600;
      cursor: pointer;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      transition: var(--transition);
    }

    .pub-toggle-btn:hover {
      color: #ffffff;
    }

    .publication-list {
      display: block; /* é»˜è®¤å±•å¼€ */
      animation: fadeIn 0.5s ease-in;
    }

    .publication-list.collapsed {
      display: none; /* æ”¶èµ·æ—¶éšè— */
    }

    @keyframes fadeIn {
      from { opacity: 0; }
      to { opacity: 1; }
    }

    /* Floating Book Widget */
    .book-widget {
      position: fixed;
      bottom: 30px;
      right: 30px;
      width: 80px;
      height: 80px;
      z-index: 1000;
      cursor: pointer;
      transition: var(--transition);
    }

    .book-widget:hover {
      transform: scale(1.1) rotate(5deg);
    }

    .book-widget img {
      width: 100%;
      height: 100%;
      border-radius: 12px;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);
    }

    /* Close Button for Book Widget */
    .close-book-btn {
      position: absolute;
      top: -6px;
      right: -6px;
      width: 20px;
      height: 20px;
      background: #ff4444;
      border-radius: 50%;
      color: white;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 12px;
      font-weight: bold;
      cursor: pointer;
      z-index: 1001;
      transition: background 0.3s;
    }

    .close-book-btn:hover {
      background: #cc0000;
    }

    /* Footer */
    footer {
      text-align: center;
      padding: 3rem 2rem 2rem;
      margin-top: 4rem;
      color: var(--text-secondary);
      font-size: 0.9rem;
      border-top: 1px solid rgba(255, 255, 255, 0.05);
    }

    @media (max-width: 768px) {
      header {
        flex-direction: column;
        text-align: center;
      }
      .links {
        justify-content: center;
      }
      .container {
        padding: 1rem;
      }
      h1 {
        font-size: 2.2rem;
      }
    }

    /* Animations */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    main > section {
      animation: fadeInUp 0.8s ease-out forwards;
    }

    main > section:nth-child(1) { animation-delay: 0.1s; }
    main > section:nth-child(2) { animation-delay: 0.2s; }
    main > section:nth-child(3) { animation-delay: 0.3s; }
    main > section:nth-child(4) { animation-delay: 0.4s; }
    main > section:nth-child(5) { animation-delay: 0.5s; }
    main > section:nth-child(6) { animation-delay: 0.6s; }
    main > section:nth-child(7) { animation-delay: 0.7s; }
  </style>
</head>
<body>
  <div class="container">

    <header>
      <img src="https://J-FHu.github.io/images/Hu_small.jpg" alt="Jinfan Hu" class="profile-img" />
      <div class="bio">
        <h1>Jinfan Hu</h1>
        <p class="affiliation">
          Ph.D. Candidate @ <a href="https://www.siat.ac.cn/" target="_blank">SIAT, CAS</a> | Advised by Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a><br>
          Researching Low-level Vision & AI Interpretability.<br><br>
          
        </p>
        <div class="links">
          <a href="mailto:jf.hu1@siat.ac.cn" class="btn"><i class="fas fa-envelope"></i> Email</a>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=hT-EiJEAAAAJ" class="btn" target="_blank"><i class="fab fa-google"></i> Scholar</a>
          <a href="https://github.com/J-FHu" class="btn" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </div>
      </div>
    </header>
    
<!-- é¡¶éƒ¨å›ºå®šå¯¼èˆªæ  -->
<nav class="top-nav">
  <div class="nav-links-group">
    <a href="#biography" class="top-nav-link">Biography</a>
    <a href="#headlines" class="top-nav-link">Headlines</a>
    <a href="#publications" class="top-nav-link">Publications</a>
    <a href="#products" class="top-nav-link">Products</a>
    <a href="#experiences" class="top-nav-link">Experiences</a>
    <a href="#honors" class="top-nav-link">Honors</a>
    <a href="#activities" class="top-nav-link">Activities</a>
  </div>
  <button class="theme-toggle-in-nav" id="themeToggle">
    <i class="fas fa-moon"></i>
  </button>
</nav>
    
    <main>
      <!-- Biography -->
      <section id="biography">
        <h2>ğŸ“š Biography</h2>
        <p><strong>Jinfan Hu</strong> is pursuing his Ph.D. degree in <a href="http://xpixel.group/index.html" target="_blank">XPixel Group</a>, Shenzhen Institute of Advanced Technology (<a href="https://www.siat.ac.cn/" target="_blank">SIAT</a>), Chinese Academy of Sciences (<a href="https://english.cas.cn/index.shtml" target="_blank">CAS</a>). He is now supervised by Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a>. He obtained his Bachelor and Master degrees in Mathematics (supervised by Prof. <a href="http://www.math.uestc.edu.cn/info/1081/2041.htm" target="_blank">Ting-Zhu Huang</a> and Prof. <a href="https://liangjiandeng.github.io/" target="_blank">Liang-Jian Deng</a>) from the University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>), Chengdu, China in 2019 and 2022. His research interests include low-level vision (LV) and AI interpretability.</p>
      </section>

<!-- Headlines -->
<section id="headlines">
  <h2>ğŸ”¥ Headlines</h2>

    <!-- HYPIR -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap;flex-direction: row-reverse;">
  <div style="flex: 1; min-width: 300px;">
    <h3>[08/2025] ğŸ‰ğŸ‰ğŸ‰ We propose HYPIR: A Step Toward the Next Generation of Image Restoration</h3>
    <p>HYPIR introduces a simple yet powerful paradigm: fine-tuning a pre-trained diffusion model with adversarial (GAN) loss â€” <strong>no diffusion sampling, no extra adapters</strong>. This achieves an unprecedented balance of speed, fidelity, and quality. Initializing GAN training from a pre-trained diffusion model anchors the model near the natural image manifold, ensuring stable training, faster convergence, and superior results â€” all in a single forward pass.</p>
    <p><strong>Official homepage:</strong> The high-speed, high-performance image restoration model is available now on <a href="https://www.suppixel.cn/home" target="_blank">æ˜çŠ€AI</a> and <a href="https://supir.suppixel.ai/home" target="_blank">SupPixel AI</a>.</p>
  </div>
  <div style="flex: 1; min-width: 300px;">
    <img src="https://J-FHu.github.io/images/hypir.png" alt="HYPIR Teaser" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
  </div>
</div>
  
  <!-- CEM Paper -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap;">
    <div style="flex: 1; min-width: 300px;">
      <h3>[03/2025] ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by TPAMI</h3>
      <p>We propose a model/task-agnostic interpreting method (<a href="https://arxiv.org/abs/2407.19789" target="_blank">CEM</a>) for low-level vision models, bringing causality analysis into the field. We hope this work contributes to a deeper understanding and enhancement of low-level vision models!</p>
      <p><strong>Key Insight:</strong> Correlation does not imply causation â€” only causal analysis reveals why.</p>
    </div>
    <div style="flex: 1; min-width: 300px;">
      <img src="https://J-FHu.github.io/images/teaser_CEM.png" alt="CEM Teaser" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
    </div>
  </div>

  <!-- Generalization Paper -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap; flex-direction: row-reverse;">
    <div style="flex: 1; min-width: 300px;">
      <h3>[02/2025] ğŸ‰ğŸ‰ğŸ‰ We revisit the generalization problem of low-level vision models</h3>
      <p>This paper demonstrates that the common strategy of blindly expanding the training set is ineffective. Instead, the key to better generalization lies in guiding the network to learn the image content rather than the degradation!</p>
      <p><strong>Takeaway:</strong> Teach networks to see the content, not just remove degradation. </p>
    </div>
    <div style="flex: 1; min-width: 300px;">
      <img src="https://J-FHu.github.io/images/Real_compare.png" alt="Generalization Comparison" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
    </div>
  </div>

  <!-- Book -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap;">
    <div style="flex: 1; min-width: 300px;">
      <h3>[02/2025] ğŸ‰ğŸ‰ğŸ‰ The book ã€Šåº•å±‚è§†è§‰ä¹‹ç¾ã€‹ has been published</h3>
      <p>Co-authored with my supervisor Prof. Chao Dong. We hope this book provides valuable insights for researchers and enthusiasts in the Low-level Vision field!</p>
      <p><strong>Available on:</strong> <a href="https://item.jd.com/10137558708716.html" target="_blank">JD.com</a></p>
    </div>
    <div style="flex: 1; min-width: 300px;">
      <img src="https://J-FHu.github.io/images/book.jpg" alt="The Beauty of Low-level Vision Book" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
    </div>
  </div>

  <!-- SUPIR -->
  <div class="pub-item" style="display: flex; align-items: center; gap: 2rem; flex-wrap: wrap; flex-direction: row-reverse;">
    <div style="flex: 1; min-width: 300px;">
      <h3>[01/2024] ğŸ‰ğŸ‰ğŸ‰ We release a groundbreaking image restoration method (SUPIR)</h3>
      <p>That harnesses the generative prior and the power of model scaling up. Our method demonstrates unprecedented performance in real-world image restoration tasks! Experience the future of image restoration through our official platforms: <a href="https://www.suppixel.cn/home" target="_blank">æ˜çŠ€AI</a> and <a href="https://supir.suppixel.ai/home" target="_blank">SupPixel AI</a>.</p>
      <p><strong>Try it:</strong> Powered by the most advanced AI servers for professional-grade results.</p>
    </div>
    <div style="flex: 1; min-width: 300px;">
      <img src="https://J-FHu.github.io/images/teaser.png" alt="SUPIR Teaser" style="width: 100%; border-radius: 12px; box-shadow: 0 8px 24px rgba(0,0,0,0.2);">
    </div>
  </div>
</section>

      <!-- Publications -->
      <section id="publications">
        <div class="pub-section-header">
          <h2>ğŸ“ Publications</h2>
          <button class="pub-toggle-btn" id="pubToggle">
            <span id="toggleIcon">â–²</span>
          </button>
        </div>
        <div class="publication-list" id="pubList">
          <h3><em>Book</em></h3>
          <div class="pub-item">
            <p>è‘£è¶…, <strong>èƒ¡é”¦å¸†</strong>, <a href="https://item.jd.com/10137558708716.html" target="_blank">ã€Šåº•å±‚è§†è§‰ä¹‹ç¾ã€‹</a>ï¼Œç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾ï¼Œ2025.</p>
          </div>

          <h3><em>Paper</em></h3>
          <div class="pub-item">
            <p>X. Lin, F. Yu, J. Hu, Z. You, W. Shi, Jimmy S. Ren, J. Gu, C. Dong. Harnessing Diffusion-Yielded Score Priors for Image Restoration. <strong><em>ACM SIGGRAPH Asia</em></strong>, 2025. <a href="https://github.com/XPixelGroup/HYPIR" target="_blank">[Code]</a><a href="https://arxiv.org/pdf/2507.20590" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J. Hu</strong>, J. Gu, S. Yao, F. Yu, Z. Li, Z. You, C. Lu, C. Dong. Interpreting Low-level Vision Models with Causal Effect Maps. <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em></strong>, 2025. <a href="https://github.com/J-FHu/CEM" target="_blank">[Code]</a><a href="https://arxiv.org/abs/2407.19789" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J. Hu<sup>*</sup></strong>, Z. You<sup>*</sup>, J. Gu, K. Zhu, T. Xue, C. Dong. Revisiting the Generalization Problem of Low-level Vision Models Through the Lens of Image Deraining. arXiv, 2025. <a href="https://arxiv.org/abs/2502.12600" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>F. Yu, J. Gu, <strong>J. Hu</strong>, Z. Li, and C. Dong. UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models. <strong><em>International Conference on Learning Representations (ICLR)</em></strong>, 2025. <a href="https://openreview.net/forum?id=uJqKf24HGN" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>F. Yu, J. Gu, Z. Li, <strong>J. Hu</strong>, X. Kong, X. Wang, J. He, Y. Qiao, and C. Dong. Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild. <strong><em>Conference on Computer Vision and Pattern Recognition (CVPR)</em></strong>, 2024. <a href="https://supir.xpixel.group/" target="_blank">[Project Page]</a><a href="https://arxiv.org/abs/2401.13627" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>R. Ran, L.-J. Deng, T.-X. Jiang, <strong>J.-F. Hu</strong>, J. Chanussot, and G. Vivone. GuidedNet: A General CNN Fusion Framework via High-resolution Guidance for Hyperspectral Image Super-resolution. <strong><em>IEEE Transactions on Cybernetics (TCYB)</em></strong>, 2023. <span class="highlight">ESI Highly Cited</span> <a href="https://github.com/Evangelion09/GuidedNet" target="_blank">[Code]</a><a href="https://ieeexplore.ieee.org/abstract/document/10035506/" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>X. Liu<sup>*</sup>, <strong>J. Hu<sup>*</sup></strong>, X. Chen, and C. Dong. UDC-UNet: Under-Display Camera Image Restoration via U-shape Dynamic Network. <strong><em>European Conference on Computer Vision Workshop (ECCVW)</em></strong>, 2022. <a href="https://github.com/J-FHu/UDCUNet" target="_blank">[Code]</a><a href="https://link.springer.com/chapter/10.1007/978-3-031-25072-9_8" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Y.-W. Zhuo, T.-J. Zhang, <strong>J.-F. Hu</strong>, H.-X. Dou, T.-Z. Huang, and L.-J. Deng. Hyper-DSNet: Hyperspectral Pansharpening via A Deep-Shallow Fusion Network with Multi-Detail Extractor and Spectral Attention. <strong><em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em></strong>, 2022. <a href="https://ieeexplore.ieee.org/abstract/document/9870551" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J.-F. Hu</strong>, T.-Z. Huang, L.-J. Deng, H.-X. Dou, D. Hong, and G. Vivone. Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution. <strong><em>IEEE Geoscience and Remote Sensing Letters</em></strong>, 2022. <span class="highlight">ESI Highly Cited</span> <a href="https://github.com/J-FHu/Fusformer" target="_blank">[Code]</a><a href="https://ieeexplore.ieee.org/abstract/document/9841513" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>S. Peng, L.-J. Deng, <strong>J.-F. Hu</strong>, and Y.-W. Zhuo. Source-Adaptive Discriminative Kernels based Network for Remote Sensing Pansharpening. <strong><em>International Joint Conferences on Artificial Intelligence (IJCAI)</em></strong>, 2022. <a href="https://github.com/liangjiandeng/ADKNet" target="_blank">[Code]</a><a href="https://pluto-wei.github.io/papers/2022/peng-ijcai2022.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p><strong>J.-F. Hu</strong>, T.-Z. Huang, L.-J. Deng, T.-X. Jiang, G. Vivone, and J. Chanussot. Hyperspectral Image Super-Resolution via Deep Spatiospectral Attention Convolutional Neural Networks. <strong><em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</em></strong>, 2021. <span class="highlight">ESI Highly Cited</span> <a href="https://liangjiandeng.github.io/Projects_Res/HSRnet_2021tnnls.html" target="_blank">[Project Page]</a><a href="https://liangjiandeng.github.io/papers/2021/HSRnet_tnnls_2021.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>T. Xu, T.-Z. Huang, L.-J. Deng, X.-L Zhao, and <strong>J.-F. Hu</strong>. Exemplar-based Image Inpainting Using Adaptive Two-Stage Structure-Tensor Based Priority Function and Nonlocal Filtering. <strong><em>Journal of Visual Communication and Image Representation</em></strong>, 2021. <a href="https://www.sciencedirect.com/science/article/abs/pii/S1047320321002893" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Z.-C. Wu, T.-Z. Huang, L.-J. Deng, <strong>J.-F. Hu</strong>, and G. Vivone. VO+ Net: An Adaptive Approach Using Variational Optimization and Deep Learning for Panchromatic Sharpening. <strong><em>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</em></strong>, 2021. <a href="https://liangjiandeng.github.io/Projects_Res/VOFF_2021tgrs.html" target="_blank">[Project page]</a><a href="https://liangjiandeng.github.io/papers/2021/VOFF.pdf" target="_blank">[PDF]</a></p>
          </div>
          <div class="pub-item">
            <p>Z.-C. Wu, T.-Z. Huang, L.-J. Deng, G. Vivone, J.-Q Miao, <strong>J.-F. Hu</strong>, and X.-L Zhao. A New Variational Approach Based on Proximal Deep Injection and Gradient Intensity Similarity for Spatio-Spectral Image Fusion. <strong><em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em></strong>, 2020. <a href="https://liangjiandeng.github.io/Projects_Res/DMPIF_2020jstars.html" target="_blank">[Project page]</a><a href="https://liangjiandeng.github.io/papers/2020/dmpif_2020jstars.pdf" target="_blank">[PDF]</a></p>
          </div>
          <p align="left"><small>(<sup>*</sup> Equal contributions)</small></p>
        </div>
      </section>

      <!-- Products -->
      <section id="products">
        <h2>ğŸ“¸ Products</h2>
        <p>For a comprehensive exploration of our technologies, visit our official websites: <a href="https://www.suppixel.cn/home" target="_blank">æ˜çŠ€AI</a> and <a href="https://supir.suppixel.ai/home" target="_blank">SupPixel AI</a>.</p>
      </section>

      <!-- Experiences -->
      <section id="experiences">
        <h2>ğŸ“– Experiences</h2>
        <ul>
          <li>09/2022-present: Ph.D. student in computer science. (Supervisor: Prof. <a href="http://xpixel.group/2010/01/20/chaodong.html" target="_blank">Chao Dong</a>); Shenzhen Institute of Advanced Technology (<a href="https://www.siat.ac.cn/" target="_blank">SIAT</a>), University of Chinese Academy of Sciences (<a href="https://www.ucas.ac.cn/" target="_blank">UCAS</a>)</li>
          <li>09/2019-06/2022: Master student in mathematics. (Supervisor: Prof. <a href="http://www.math.uestc.edu.cn/info/1081/2041.htm" target="_blank">Ting-Zhu Huang</a> and Prof. <a href="https://liangjiandeng.github.io/" target="_blank">Liang-Jian Deng</a>); University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>)</li>
          <li>09/2015-06/2019: Bachelor student in information and computing science; University of Electronic Science and Technology of China (<a href="https://www.uestc.edu.cn/" target="_blank">UESTC</a>)</li>
        </ul>
      </section>

      <!-- Honors and Awards -->
      <section id="honors">
        <h2>ğŸ† Honors and Awards</h2>
        <ul>
          <li>2nd Place in ECCV <a href="http://mipi-challenge.org/" target="_blank">MIPI 2022 Challenge</a> on UDC image restoration, 2022</li>
          <li>National Scholarship, 2021</li>
          <li>National First Prize of <a href="http://www.mcm.edu.cn/" target="_blank">CUMCM</a>, 2017</li>
        </ul>
      </section>

      <!-- Academic Activities -->
      <section id="activities">
        <h2>ğŸ’¬ Academic Activities</h2>
        <p><strong>Peer-Reviewer:</strong></p>
        <ul>
          <li>IEEE Transactions on Pattern Analysis and Machine Intelligence</li>
          <li>IEEE Transactions on Cybernetics</li>
          <li>IEEE Transactions on Geoscience And Remote Sensing</li>
          <li>IEEE Geoscience and Remote Sensing Letters</li>
          <li>IEEE Transactions on Computational Imaging</li>
          <li>...</li>
        </ul>
      </section>
    </main>

    <footer>
      Â© 2025 Jinfan Hu. Built with QWen.
    </footer>

    
<!-- Floating Book Widget with Close Button -->
<div class="book-widget" id="bookWidget">
  <div class="close-book-btn" id="closeBookBtn">Ã—</div>
  <a href="https://item.jd.com/10137558708716.html  " target="_blank">
    <img src="https://J-FHu.github.io/images/bookicon.jpg  " alt="ã€Šåº•å±‚è§†è§‰ä¹‹ç¾ã€‹">
  </a>
</div>

<style>
/* ========== é¡¶éƒ¨å›ºå®šå¯¼èˆªæ  ========== */
.top-nav {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  background: rgba(10, 20, 40, 0.9);
  backdrop-filter: blur(10px);
  padding: 0 2rem;
  height: 60px;
  display: flex;
  align-items: center;
  z-index: 1000;
  border-bottom: 1px solid rgba(0, 200, 255, 0.2);
  transition: var(--transition);
}

body[data-theme="light"] .top-nav {
  background: rgba(255, 255, 255, 0.9);
  border-bottom: 1px solid rgba(13, 110, 253, 0.2);
}

/* é“¾æ¥ç»„ - å æ®å‰©ä½™ç©ºé—´å¹¶å±…ä¸­ */
.nav-links-group {
  display: flex;
  gap: 2rem;
  flex-grow: 1; /* å æ®å‰©ä½™ç©ºé—´ */
  justify-content: center; /* å†…éƒ¨é“¾æ¥å±…ä¸­ */
}

/* ä¸»é¢˜åˆ‡æ¢æŒ‰é’® - é å³ */
.theme-toggle-in-nav {
  background: none;
  border: none;
  width: 40px;
  height: 40px;
  border-radius: 50%;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  color: var(--text-secondary);
  transition: var(--transition);
  margin-left: auto; /* å…³é”®ï¼šå°†æŒ‰é’®æ¨åˆ°æœ€å³è¾¹ */
}

.theme-toggle-in-nav:hover {
  color: var(--accent);
  background: rgba(0, 200, 255, 0.1);
  transform: scale(1.05);
}

body[data-theme="light"] .theme-toggle-in-nav {
  color: var(--text-secondary);
}

body[data-theme="light"] .theme-toggle-in-nav:hover {
  color: var(--accent);
  background: rgba(13, 110, 253, 0.1);
}

.top-nav-link {
  color: var(--text-secondary);
  text-decoration: none;
  font-size: 0.95rem;
  font-weight: 500;
  padding: 0.5rem 0.8rem;
  border-radius: 6px;
  transition: var(--transition);
}

.top-nav-link:hover,
.top-nav-link.active {
  color: var(--accent);
  background: rgba(0, 200, 255, 0.1);
}

/* è®©ä¸»å†…å®¹åŒºåŸŸä¸ºé¡¶éƒ¨å¯¼èˆªç•™å‡ºç©ºé—´ */
main {
  margin-top: 80px; /* 60px (å¯¼èˆªæ ) + 20px (é—´è·) */
}

/* æ‰‹æœºç«¯é€‚é… */
@media (max-width: 768px) {
  .top-nav {
    padding: 0 1rem;
    gap: 1rem;
    height: 50px;
    overflow-x: auto;
    white-space: nowrap;
  }

  .top-nav-link {
    font-size: 0.9rem;
    padding: 0.4rem 0.6rem;
  }

  main {
    margin-top: 70px;
  }
}
  
  /* Floating Book Widget */
  .book-widget {
    position: fixed;
    bottom: 30px;
    right: 30px;
    width: 100px; 
    height: 100px; 
    z-index: 1000;
    cursor: pointer;
    transition: var(--transition);
  }

  .book-widget:hover {
    transform: scale(1.1) rotate(5deg);
  }

  .book-widget img {
    width: 100%;
    height: 100%;
    border-radius: 12px;
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);
  }

  /* Close Button for Book Widget */
  .close-book-btn {
    position: absolute;
    top: -10px;
    right: -10px;
    width: 35px;
    height: 35px;
    background: #ff4444;
    border-radius: 50%;
    color: white;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 18px;
    font-weight: bold;
    cursor: pointer;
    z-index: 1001;
    transition: background 0.3s;
    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  }

  .close-book-btn:hover {
    background: #cc0000;
    transform: scale(1.1);
  }
</style>

    <!-- ClustrMaps Visitor Counter -->
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=5Tnjvlx4pwYiXFQMdiOgvLHFKUuOMdlwnkZD9DMfS6c'></script>

    <!-- Main JavaScript -->
    <script>
      // é¡¶éƒ¨å¯¼èˆªæ é«˜äº®è„šæœ¬
document.addEventListener('DOMContentLoaded', function() {
  const navLinks = document.querySelectorAll('.top-nav-link');
  const sections = document.querySelectorAll('section');

  // å¹³æ»‘æ»šåŠ¨
  navLinks.forEach(link => {
    link.addEventListener('click', function(e) {
      e.preventDefault();
      const targetId = this.getAttribute('href');
      const targetSection = document.querySelector(targetId);
      if (targetSection) {
        window.scrollTo({
          top: targetSection.offsetTop - 80, // å‡å»å¯¼èˆªæ é«˜åº¦
          behavior: 'smooth'
        });
      }
    });
  });

  // æ»šåŠ¨é«˜äº®
  window.addEventListener('scroll', function() {
    let current = '';
    sections.forEach(section => {
      const sectionTop = section.offsetTop;
      if (pageYOffset >= sectionTop - 200) {
        current = '#' + section.getAttribute('id');
      }
    });

    navLinks.forEach(link => {
      link.classList.remove('active');
      if (link.getAttribute('href') === current) {
        link.classList.add('active');
      }
    });
  });
});
      
      // Theme Toggle Script
      const themeToggle = document.getElementById('themeToggle');
      const body = document.body;

      themeToggle.addEventListener('click', () => {
        const currentTheme = body.getAttribute('data-theme');
        if (currentTheme === 'light') {
          body.removeAttribute('data-theme');
          localStorage.setItem('theme', 'dark');
          themeToggle.innerHTML = '<i class="fas fa-moon"></i>';
        } else {
          body.setAttribute('data-theme', 'light');
          localStorage.setItem('theme', 'light');
          themeToggle.innerHTML = '<i class="fas fa-sun"></i>';
        }
      });

      // Publication Toggle Script
      const pubToggle = document.getElementById('pubToggle');
  const pubList = document.getElementById('pubList');

  pubToggle.addEventListener('click', () => {
    pubList.classList.toggle('collapsed'); 
    if (pubList.classList.contains('collapsed')) {
      pubToggle.innerHTML = '<span id="toggleIcon">â–¼</span>';
    } else {
      pubToggle.innerHTML = '<span id="toggleIcon">â–²</span>';
    }
  });

      // Book Widget Close Script
      const closeBookBtn = document.getElementById('closeBookBtn');
      const bookWidget = document.getElementById('bookWidget');

      closeBookBtn.addEventListener('click', function(e) {
        e.stopPropagation(); //
        bookWidget.style.display = 'none';
      });

      // é¡µé¢åŠ è½½æ—¶æ£€æŸ¥æ˜¯å¦ä¹‹å‰å·²å…³é—­
      if (localStorage.getItem('bookWidgetClosed') === 'true') {
        bookWidget.style.display = 'none';
      }
    </script>
  </div>
</body>
</html>
